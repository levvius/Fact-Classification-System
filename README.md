# Fact Classification System

REST API Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° ĞºĞ°Ğº "Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ°", "Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ´Ğ°" Ğ¸Ğ»Ğ¸ "Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾" Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NLI (Natural Language Inference) Ğ¸ Wikipedia.

## ğŸš€ Quick Start

```bash
# 1. Activate virtual environment (Ğ’ĞĞ–ĞĞ!)
source venv/bin/activate

# 2. (First time only) Build Knowledge Base
python scripts/build_kb.py

# 3. Start the server
./run.sh

# 4. Open browser
# http://localhost:8000
```

**Ğ’ĞĞ–ĞĞ**: Ğ’ÑĞµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Python Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒÑÑ Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼!

---

## âœ¨ Features

### Web Interface (NEW!)
- ğŸ¨ Modern, responsive web UI
- ğŸ“š Browse 18 Wikipedia topics across 4 categories
- ğŸ” Real-time fact classification
- ğŸ“Š Detailed results with evidence from Wikipedia
- âœ… Comprehensive error handling

### API Features
- ğŸ§  Natural Language Inference (RoBERTa-large-mnli)
- ğŸ” FAISS vector search for evidence retrieval
- ğŸ“ Automatic claim extraction from text
- ğŸŒ 265 Wikipedia articles in Knowledge Base
- ğŸš¦ Rate limiting (10 req/min)
- ğŸ’¾ Response caching (5-minute TTL)
- ğŸ”’ XSS validation and input sanitization

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.9-3.13 (recommended: 3.13.1)
- pip
- Virtual environment (venv)

### Step-by-Step Setup

```bash
# 1. Clone the repository
git clone <repository-url>
cd IS-hallucination-detection

# 2. Create virtual environment
python3 -m venv venv

# 3. Activate virtual environment
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 4. Install dependencies
pip install -r requirements.txt

# 5. Build Knowledge Base (takes 2-5 minutes)
python scripts/build_kb.py
```

**Verification**: After successful setup, these files should exist:
- `data/faiss_index/wikipedia.index` (FAISS index, ~400KB)
- `data/kb_snippets.json` (metadata, ~145KB)

---

## ğŸ¯ Usage

### Web Interface

1. **Start the server**:
   ```bash
   source venv/bin/activate  # Always activate first!
   ./run.sh
   ```

2. **Open browser**:
   ```
   http://localhost:8000
   ```

3. **Use the interface**:
   - Browse available topics (People, Technology, Science, History & Geography)
   - Click a topic to insert an example fact
   - Enter your own text (10-5000 characters)
   - Click "Classify Text"
   - View results with evidence

**Expected behavior**:
- First request: 5-10 seconds (models loading)
- Subsequent requests: 3-5 seconds (models cached)
- Green status indicator: API Ready
- Red status indicator: Models loading or error

### API Usage

#### Health Check
```bash
curl http://localhost:8000/api/v1/health
```

Response:
```json
{
  "status": "healthy",
  "models_loaded": true,
  "kb_size": 265
}
```

#### Classify Text
```bash
curl -X POST http://localhost:8000/api/v1/classify \
  -H "Content-Type: application/json" \
  -d '{"text": "Albert Einstein was born in 1879 and won the Nobel Prize in Physics."}'
```

Response:
```json
{
  "overall_classification": "Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ°",
  "confidence": 0.95,
  "claims": [
    {
      "claim": "Albert Einstein was born in 1879.",
      "classification": "Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ°",
      "confidence": 0.99,
      "best_evidence": {
        "snippet": "Albert Einstein was born in Ulm...",
        "source": "https://en.wikipedia.org/wiki/Albert_Einstein",
        "nli_score": 0.99,
        "retrieval_score": 0.98
      }
    }
  ]
}
```

#### Get Available Topics
```bash
curl http://localhost:8000/api/v1/topics
```

---

## ğŸ” Troubleshooting

### 1. ModuleNotFoundError: sentence_transformers

**ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°**: Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ**:
```bash
source venv/bin/activate
python scripts/build_kb.py  # Now it will work
```

### 2. Network Error on Classify Button

**ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°**: API ÑĞµÑ€Ğ²ĞµÑ€ Ğ½Ğµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ**:
```bash
source venv/bin/activate
./run.sh  # Start the server
```

Ğ”Ğ¾Ğ¶Ğ´Ğ¸Ñ‚ĞµÑÑŒ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ:
```
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
âœ“ Models loaded successfully
```

### 3. Models Not Loaded (503 Error)

**ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°**: ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ĞµÑ‰Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ÑÑ‚ÑÑ (Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº)

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ**: ĞŸĞ¾Ğ´Ğ¾Ğ¶Ğ´Ğ¸Ñ‚Ğµ 5-10 ÑĞµĞºÑƒĞ½Ğ´ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° ÑĞµÑ€Ğ²ĞµÑ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸.

### 4. Knowledge Base Missing

**ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°**: `data/faiss_index/wikipedia.index` Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ**:
```bash
source venv/bin/activate
python scripts/build_kb.py  # Rebuild KB (2-5 minutes)
```

### 5. Rate Limit Exceeded (429 Error)

**ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°**: ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ 10 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ**: ĞŸĞ¾Ğ´Ğ¾Ğ¶Ğ´Ğ¸Ñ‚Ğµ 60 ÑĞµĞºÑƒĞ½Ğ´ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ ÑĞµÑ€Ğ²ĞµÑ€

### 6. Port 8000 Already in Use

**ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°**: Ğ”Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ñ‚ 8000

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ**:
```bash
# Find and kill the process
kill $(lsof -t -i:8000)

# Then restart
./run.sh
```

---

## ğŸ§ª Testing

### Unit Tests (90 tests, ~5 seconds)
```bash
source venv/bin/activate
pytest tests/unit -m unit
```

### Integration Tests (16 tests, ~60 seconds)
```bash
source venv/bin/activate
pytest tests/integration -m integration
```

### All Tests with Coverage
```bash
source venv/bin/activate
pytest tests/ --cov=app --cov-report=html
```

---

## ğŸ“ Project Structure

```
IS-hallucination-detection/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI application
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py              # API endpoints (/classify, /health, /topics)
â”‚   â”‚   â””â”€â”€ schemas.py             # Pydantic models
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py              # Configuration
â”‚   â”‚   â”œâ”€â”€ models.py              # ModelManager singleton
â”‚   â”‚   â”œâ”€â”€ cache.py               # Response caching
â”‚   â”‚   â””â”€â”€ exceptions.py          # Custom exceptions
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ claim_extractor.py    # Extract claims from text
â”‚   â”‚   â”œâ”€â”€ evidence_retriever.py # FAISS search
â”‚   â”‚   â”œâ”€â”€ nli_verifier.py       # NLI scoring
â”‚   â”‚   â””â”€â”€ classifier.py         # Main classification logic
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ wikipedia_kb.py       # KB building utilities
â”‚   â””â”€â”€ static/                    # Frontend files (NEW!)
â”‚       â”œâ”€â”€ index.html             # Main UI
â”‚       â”œâ”€â”€ css/styles.css         # Responsive design
â”‚       â””â”€â”€ js/
â”‚           â”œâ”€â”€ api.js             # API client
â”‚           â”œâ”€â”€ ui.js              # UI controller
â”‚           â””â”€â”€ app.js             # Main logic
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ build_kb.py                # Build Knowledge Base
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                      # 90 unit tests
â”‚   â””â”€â”€ integration/               # 16 integration tests
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ faiss_index/               # FAISS vector index
â”‚   â””â”€â”€ kb_snippets.json           # KB metadata
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ run.sh                         # Startup script
â””â”€â”€ README.md                      # This file
```

---

## âš™ï¸ Configuration

Configuration is managed via `app/core/config.py`:

### Model Configuration
- `EMBED_MODEL`: `all-MiniLM-L6-v2` (sentence embeddings)
- `NLI_MODEL`: `roberta-large-mnli` (NLI scoring)

### Classification Thresholds
- `TRUTH_THRESHOLD`: 0.85 (>= 85% confidence = Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ°)
- `FALSEHOOD_THRESHOLD`: 0.4 (< 40% confidence = Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ´Ğ°)

### Retrieval Settings
- `TOP_K_PROOFS`: 6 (retrieve top 6 evidence snippets)
- `MAX_CLAIMS`: 8 (max claims to extract)

### API Settings
- `RATE_LIMIT_REQUESTS`: 10 (requests per minute)
- `CACHE_TTL`: 300 seconds (5 minutes)
- `CACHE_MAX_SIZE`: 100 entries

---

## ğŸ“– How It Works

### Architecture Overview

```
User Input (English text)
    â†“
1. Claim Extraction
   - Split text into sentences
   - Extract factual claims
    â†“
2. Evidence Retrieval
   - FAISS vector search
   - Find top 6 relevant Wikipedia snippets
    â†“
3. NLI Verification
   - RoBERTa-large-mnli model
   - Score claim-evidence entailment
    â†“
4. Classification
   - Aggregate NLI scores
   - Apply thresholds (0.85/0.4)
   - Return verdict: Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ°/Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ´Ğ°/Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾
```

### Classification Logic

**Per-claim scoring:**
- `support >= 0.85` â†’ "Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ°" (high confidence)
- `0.4 <= support < 0.85` â†’ "Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾" (uncertain)
- `support < 0.4` â†’ "Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ´Ğ°" (contradicts evidence)

**Overall aggregation** (pessimistic):
- ANY claim "Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ´Ğ°" â†’ overall "Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ´Ğ°"
- Else, ANY claim "Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾" â†’ overall "Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾"
- Else â†’ overall "Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ°"

---

## ğŸ¤ Contributing

This is a university project for "Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼".

**Course**: Information Systems Design and Maintenance Technologies
**University**: [Your University Name]
**Year**: 2025

---

## ğŸ“ Support

For issues, please check:
1. [Troubleshooting](#troubleshooting) section above
2. Server logs (`uvicorn` output in terminal)
3. Browser console (F12) for frontend errors

---

## ğŸ”— Additional Resources

- **API Documentation**: http://localhost:8000/docs (Swagger UI)
- **Health Check**: http://localhost:8000/api/v1/health
- **Frontend**: http://localhost:8000
- **Project Documentation**: See `CLAUDE.md` for detailed architecture

---

## ğŸ“ Recent Updates

### Version 2.0 (Current)
- âœ… Added web interface (HTML/CSS/JavaScript)
- âœ… 18 Wikipedia topics with examples
- âœ… Improved error handling with clear messages
- âœ… Environment checks in build scripts
- âœ… Better startup experience

### Version 1.0
- âœ… REST API with FastAPI
- âœ… NLI-based fact verification
- âœ… FAISS vector search
- âœ… Wikipedia knowledge base
- âœ… Comprehensive testing (106 tests)

---

**Made with â¤ï¸ for accurate fact verification**
